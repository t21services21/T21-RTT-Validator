# ü§ñ HOW THE JOB AUTOMATION SYSTEM WORKS

## ‚úÖ **WHAT YOU JUST DID (STEP 1):**

You successfully **ACTIVATED** automation for student Tosin!

```
‚úÖ Student added successfully!
‚úÖ Automation is now ACTIVE for Tosin
‚úÖ Credentials encrypted and stored
‚úÖ Preferences saved
```

---

## üîÑ **WHAT HAPPENS NEXT (AUTOMATIC):**

The job automation runs in **3 BACKEND PROCESSES** that you don't see:

### **PROCESS 1: JOB SCRAPER (Every 6 Hours)**
```
1. System scrapes NHS Jobs website
2. Finds matching jobs based on:
   - Student's preferred locations (London, Nottingham, Manchester, Liverpool)
   - Student's preferred bands (Band 3, Band 4)
   - Sponsorship requirement (Yes)
3. Stores jobs in "discovered_jobs" table
```

**File:** `job_automation/scraper_engine.py`

### **PROCESS 2: AI APPLICATION GENERATOR (Every 1 Hour)**
```
1. System gets jobs from "discovered_jobs" table
2. For each matching job:
   - Generates unique supporting information using GPT-4
   - Tailors content to job description
   - Includes student's T21 qualifications
3. Creates application record in "applications" table
4. Queues for submission
```

**File:** `job_automation/ai_generator.py`

### **PROCESS 3: AUTO-SUBMITTER (Every 30 Minutes)**
```
1. System checks "applications" table for queued items
2. Logs into NHS Trac with student credentials
3. Submits application with AI-generated content
4. Gets confirmation number
5. Updates status to "submitted"
6. Sends email notification to student
```

**File:** `job_automation/trac_auto_submitter.py`

---

## üìä **WHERE TO SEE RESULTS:**

### **For Staff (You):**
```
Career Development ‚Üí Job Automation ‚Üí Manage Students tab

You'll see:
- Student status (Active/Paused)
- Trac email
- Preferences
- Applications submitted (count)
- Interviews invited (count)
```

### **For Students (Tosin):**
```
Career Development ‚Üí Job Automation

Student sees:
- Dashboard with statistics
- All applications submitted
- Interview invitations
- Job offers
```

---

## ‚ö†Ô∏è **WHY YOU DON'T SEE JOBS YET:**

**The backend processes haven't run yet because:**

1. ‚ùå **Scraper engine not scheduled** (needs cron job or background worker)
2. ‚ùå **AI generator not running** (needs OpenAI API key configured)
3. ‚ùå **Auto-submitter not running** (needs to be scheduled)

**These are BACKEND PROCESSES that need to be set up separately!**

---

## üîß **TO MAKE IT ACTUALLY WORK:**

### **Option 1: Manual Testing (Test if it works)**
```python
# Run manually in terminal or Jupyter notebook
from job_automation.scraper_engine import scrape_nhs_jobs
from job_automation.ai_generator import generate_applications
from job_automation.trac_auto_submitter import submit_queued_applications

# 1. Scrape jobs
jobs = scrape_nhs_jobs(
    locations=['London', 'Nottingham'],
    bands=['Band 3', 'Band 4'],
    requires_sponsorship=True
)
print(f"Found {len(jobs)} matching jobs")

# 2. Generate applications
generate_applications(student_id='student-uuid-here')

# 3. Submit applications
submit_queued_applications()
```

### **Option 2: Schedule Backend Processes (Production)**

**Using Celery + Redis:**
```python
# celery_tasks.py
from celery import Celery
from job_automation.scraper_engine import scrape_all_students_jobs
from job_automation.ai_generator import generate_all_applications
from job_automation.trac_auto_submitter import submit_all_queued

app = Celery('job_automation')

@app.task
def scrape_jobs_task():
    scrape_all_students_jobs()

@app.task
def generate_apps_task():
    generate_all_applications()

@app.task
def submit_apps_task():
    submit_all_queued()

# Schedule tasks
app.conf.beat_schedule = {
    'scrape-every-6-hours': {
        'task': 'scrape_jobs_task',
        'schedule': 3600 * 6  # Every 6 hours
    },
    'generate-every-hour': {
        'task': 'generate_apps_task',
        'schedule': 3600  # Every hour
    },
    'submit-every-30-min': {
        'task': 'submit_apps_task',
        'schedule': 1800  # Every 30 minutes
    }
}
```

**Start workers:**
```bash
celery -A celery_tasks worker --loglevel=info
celery -A celery_tasks beat --loglevel=info
```

### **Option 3: Use Streamlit Cloud + GitHub Actions (Easier)**

**Create `.github/workflows/job_automation.yml`:**
```yaml
name: NHS Job Automation

on:
  schedule:
    - cron: '0 */6 * * *'  # Every 6 hours

jobs:
  scrape_and_apply:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      
      - name: Set up Python
        uses: actions/setup-python@v2
        with:
          python-version: 3.9
      
      - name: Install dependencies
        run: |
          pip install -r requirements.txt
      
      - name: Scrape NHS Jobs
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_KEY: ${{ secrets.SUPABASE_KEY }}
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        run: |
          python -c "from job_automation.scraper_engine import scrape_all_students_jobs; scrape_all_students_jobs()"
      
      - name: Generate Applications
        run: |
          python -c "from job_automation.ai_generator import generate_all_applications; generate_all_applications()"
      
      - name: Submit Applications
        run: |
          python -c "from job_automation.trac_auto_submitter import submit_all_queued; submit_all_queued()"
```

---

## üí° **FOR NOW (TESTING):**

**You can simulate the workflow:**

1. **Manually add a job** to `discovered_jobs` table in Supabase:
```sql
INSERT INTO discovered_jobs (
    title, trust, location, band, 
    salary_min, salary_max, requires_sponsorship,
    closing_date, nhs_jobs_url
) VALUES (
    'RTT Validation Officer', 'Royal London Hospital', 'London', 'Band 3',
    24000, 28000, true,
    NOW() + INTERVAL '14 days', 'https://jobs.nhs.uk/example'
);
```

2. **Check if student sees it** in their dashboard

3. **Manually create application record:**
```sql
INSERT INTO applications (
    student_id, job_id, status, ai_supporting_information
) VALUES (
    'tosin-uuid-here', 'job-uuid-here', 'submitted', 
    'I am interested in this role because...'
);
```

4. **Student will see application** in their dashboard!

---

## üìß **EMAIL NOTIFICATIONS:**

**Currently not implemented yet!**

Need to add SendGrid integration:
```python
# job_automation/email_notifications.py
from sendgrid import SendGridAPIClient
from sendgrid.helpers.mail import Mail

def send_application_notification(student_email, job_title, trust):
    message = Mail(
        from_email='admin@t21services.co.uk',
        to_emails=student_email,
        subject=f'Application Submitted: {job_title}',
        html_content=f'''
        <h2>Job Application Submitted!</h2>
        <p>Your application to <strong>{job_title}</strong> at <strong>{trust}</strong> has been submitted successfully.</p>
        <p>We'll notify you when there's an update!</p>
        '''
    )
    
    sg = SendGridAPIClient(os.environ.get('SENDGRID_API_KEY'))
    sg.send(message)
```

---

## üéØ **SUMMARY:**

**What you have now:**
‚úÖ Student settings stored
‚úÖ Automation activated
‚úÖ Database ready
‚úÖ UI for staff and students

**What's missing (backend):**
‚ùå Scraper running automatically
‚ùå AI generator running
‚ùå Auto-submitter running
‚ùå Email notifications

**To test manually:**
1. Add fake job to database
2. Create fake application
3. Student will see it in dashboard

**To make it fully automatic:**
1. Set up Celery workers, OR
2. Set up GitHub Actions, OR
3. Set up cloud scheduler (AWS Lambda, Google Cloud Functions)

---

## üíº **BUSINESS DECISION NEEDED:**

**Which backend approach do you want?**

**Option A: Celery + Redis** (Professional, scalable)
- Pros: Real-time, powerful, professional
- Cons: Requires server, Redis setup, more complex

**Option B: GitHub Actions** (Simple, free)
- Pros: Free, easy to set up, no server needed
- Cons: Runs on schedule only, not real-time

**Option C: Cloud Functions** (Serverless, scalable)
- Pros: Scales automatically, pay per use
- Cons: Costs money, vendor lock-in

**Option D: Manual (For testing only)**
- Pros: Free, full control
- Cons: Not automatic, admin must trigger

---

**Let me know which approach you want and I'll implement it!** üöÄ
